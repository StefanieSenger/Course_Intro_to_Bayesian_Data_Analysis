pnorm(-2)
library(extraDistr)
pnorm(-2)
pnorm(2, lower.tail = FALSE=
pnorm(2, lower.tail = FALSE)
pnorm(2.5, lower.tail = FALSE)
variable <- rbern(n=100, prob=0.5)
variable <- rbern(n=100, prob=0.5)
varibale
variable
# Question: "We ask a subject 10 yes/no questions, and the subject returns 0 correct answers.
# We assume a binomial likelihood function for this data. We also assume a Beta(1, 1)
# prior on the parameter θ, which represents the probability of success.
# Write down the posterior distribution of the θ parameter.
prior <- dbeta(x = 0:1, shape1 = 1, shape2 = 1) #Define the prior distribution as Beta(1, 1)
prior
likelihood <- dbinom(0, size = 10, prob = x) # Define the likelihood function as a binomial distribution
likelihood
likelihood <- dbinom(0, size = 10, prob = 0:1) # Define the likelihood function as a binomial distribution
likelihood
post <- prior * likelihood # Calculate the posterior distribution
post
post <- post / sum(post) # Normalize the posterior distribution
post
x <- seq(0, 1, length.out = 100)
plot(x, prior, type = "l", xlab = "Theta", ylab = "Density",
main = "Prior and Posterior Distributions")
lines(x, post, col = "red")
legend("topright", c("Prior", "Posterior"), col = c("black", "red"),
lty = c(1, 1), cex = 0.8)
qbeta(c(0.025, 0.975), shape1 = 19, shape2 = 2)
# Question: Assuming a binomial likelihood, you are given the following data:
# 18 successes out of 100 independent trials. Moreover, suppose the prior on
# the θ parameter in the binomial distribution is a beta distribution with
# a = 15 and b = 15. Find the kernel of the posterior distribution of θ.
prior <- dbeta(x = 0:1, shape1 = 15, shape2 = 15)
likelihood <- dbinom(18, size = 100, prob = 0:1)
post <- prior * likelihood
post <- post / sum(post)
post
# Question: Assuming a binomial likelihood, you are given the following data:
# 18 successes out of 100 independent trials. Moreover, suppose the prior on
# the θ parameter in the binomial distribution is a beta distribution with
# a = 15 and b = 15. Find the kernel of the posterior distribution of θ.
prior <- dbeta(x = 0:1, shape1 = 15, shape2 = 15)
prior
likelihood <- dbinom(18, size = 100, prob = 0:1)
likelihood
post <- prior * likelihood
post
post <- post / sum(post)
post
qbeta(c(0.025, 0.975), shape1 = 33, shape2 = 97)
# drawing inferences from posterior distribitions analytically (in this example within 95% credible interval)
qgamma(c(0.025, 0.975), shape=20, rate=7)
# but we can also sample from the posterior distribution and thus derive the same conclusions:
lambda_posteror <- rgamma(c(0.025, 0.975), shape=20, rate=7)
lambda_posteror
quantile(lambda_posteror)
# but we can also sample from the posterior distribution and thus derive the same conclusions:
lambda_posteror <- rgamma(c(0.025, 0.975), shape=20, rate=7)
# but we can also sample from the posterior distribution and thus derive the same conclusions:
lambda_posteror <- rgamma(c(0.025, 0.975), shape=20, rate=7)
lambda_posteror
# but we can also sample from the posterior distribution and thus derive the same conclusions:
lambda_posteror <- rgamma(c(0.025, 0.975), shape=20, rate=7)
lambda_posteror
# but we can also sample from the posterior distribution and thus derive the same conclusions:
lambda_posteror <- rgamma(c(0.025, 0.975), shape=20, rate=7)
lambda_posteror
# but we can also sample from the posterior distribution and thus derive the same conclusions:
lambda_posteror <- rgamma(c(0.025, 0.975), shape=20, rate=7)
lambda_posteror
# but we can also sample from the posterior distribution and thus derive the same conclusions:
lambda_posteror <- rgamma(c(0.025, 0.975), shape=20, rate=7)
lambda_posteror
quantile(lambda_posteror)
quantile(lambda_posteror, probs = c(0.025, 0.975))
# but we can also sample from the posterior distribution and thus derive the same conclusions:
lambda_posteror <- rgamma(c(0.025, 0.975), shape=20, rate=7) # saving sampled quantiles in lambda_posteror
lambda_posteror
quantile(lambda_posteror, probs = c(0.025, 0.975))
# but we can also sample from the posterior distribution and thus derive the same conclusions:
lambda_posteror <- rgamma(4000, shape=20, rate=7) # saving sampled quantiles in lambda_posteror
lambda_posteror
# but we can also sample from the posterior distribution and thus derive the same conclusions:
lambda_posteror <- rgamma(4000, shape=20, rate=7) # saving sampled quantiles in lambda_posteror
lambda_posteror
quantile(lambda_posteror, probs = c(0.025, 0.975))
# example: simple linear model
library(bcogsci)
install.packages("bcogsci")
# example: simple linear model
library(bcogsci)
install.packages("bcogsci")
install.packages("devtools")
library(devtools)
library(devtools)
package.install(bcogsci)
install.packages(bcogsci)
install.packages("bcogsci")
install.packages("bcogsci")
install.packages("remotes")
remotes::install_github("bnicenboim/bcogsci")
# example: simple linear model
library(bcogsci)
data("df_spacebar")
head(df_spacebar, n=2)
head(df_spacebar, n=10)
head(df_spacebar, n=2)
# frequentist linear model
m <-lm(rt~1, df_spacebar)
m
sigma(m)
coef(m) # intercept
sigma(m)
mean(df_spacebar)
mean(df_spacebar$rt)
sd(df_spacebar$rt)
# starting with unrealistic priors from a uniform distribution
load(brms)
install.packages(brms)
install.packages("brms")
# starting with unrealistic priors from a uniform distribution
load(brms)
# starting with unrealistic priors from a uniform distribution
load(brms::brm)
# starting with unrealistic priors from a uniform distribution
load(brm)
load(brms)
# starting with unrealistic priors from a uniform distribution
library(brms)
fit_press <- brm(rt~1,
data=df_spacebar,
family=gaussian(), # likelihood we're assuming for the data shall be the gaussian likelihood (normal distribution)
prior=c(
prior(uniform(0,60000), class=Intercept, lb=0, ub=60000), # assuming mu (mean) comes from a prior uniform distribution between 0 and 60000
prior(uniform(0,2000), class=sigma, lb=0, ub=2000) # assuming sigma (sd) comes from a prior uniform distribution between 0 and 2000
),
chains= 4,
iter=2000
warmup=1000
)
# starting with unrealistic priors from a uniform distribution
library(brms)
fit_press <- brm(rt~1,
data=df_spacebar,
family=gaussian(), # likelihood we're assuming for the data shall be the gaussian likelihood (normal distribution)
prior=c(
prior(uniform(0,60000), class=Intercept, lb=0, ub=60000), # assuming mu (mean) comes from a prior uniform distribution between 0 and 60000
prior(uniform(0,2000), class=sigma, lb=0, ub=2000) # assuming sigma (sd) comes from a prior uniform distribution between 0 and 2000
),
chains= 4,
iter=2000,
warmup=1000
)
plot(fit_press)
as_draws_df(fit_press) %>% head(3)
installed.packages("magrittr")
install.packages("magrittr")
install.packages("magrittr")
install.packages("magrittr")
install.packages("magrittr")
library(magrittr)
as_draws_df(fit_press) %>% head(3)
install.packages("bayesplot")
install.packages("bayesplot")
install.packages("bayesplot")
library(bayesplot)
library(magrittr)
as_draws_df(fit_press) %>% head(3)
library(bayesplot)
library(magrittr)
as_draws_df(fit_press) %>% head(3)
# starting with unrealistic priors from a uniform distribution
library(brms)
fit_press <- brm(rt~1,
data=df_spacebar,
family=gaussian(), # likelihood we're assuming for the data shall be the gaussian likelihood (normal distribution)
prior=c(
prior(uniform(0,60000), class=Intercept, lb=0, ub=60000), # assuming mu (mean) comes from a prior uniform distribution between 0 and 60000
prior(uniform(0,2000), class=sigma, lb=0, ub=2000) # assuming sigma (sd) comes from a prior uniform distribution between 0 and 2000
),
chains= 4, # number of independent runs for sampling, default is 4
iter=2000, # rumber of iterations that the sampler makes to sample from the posterior distribution, default is 2000
warmup=1000 # number of iterations from the start of sampling that are eventually discarded, default is 1000 (half of iter)
)
# plotting posterior distributions of mu and sigma (and the chains)
plot(fit_press)
library(bayesplot)
library(magrittr)
as_draws_df(fit_press) %>% head(3)
# starting with unrealistic priors from a uniform distribution
library(brms)
fit_press <- brm(rt~1,
data=df_spacebar,
family=gaussian(), # likelihood we're assuming for the data shall be the gaussian likelihood (normal distribution)
prior=c(
prior(uniform(0,60000), class=Intercept, lb=0, ub=60000), # assuming mu (mean) comes from a prior uniform distribution between 0 and 60000
prior(uniform(0,2000), class=sigma, lb=0, ub=2000) # assuming sigma (sd) comes from a prior uniform distribution between 0 and 2000
),
chains= 4, # number of independent runs for sampling, default is 4
iter=2000, # rumber of iterations that the sampler makes to sample from the posterior distribution, default is 2000
warmup=1000 # number of iterations from the start of sampling that are eventually discarded, default is 1000 (half of iter)
)
# drawing inferences from posterior distribitions analytically (in this example within 95% credible interval)
qgamma(c(0.025, 0.975), shape=20, rate=7) # shape and rate are the a and b parameter from the gamma distribution
# alternative: we can also sample from the posterior distribution and thus derive the same conclusions:
lambda_posteror <- rgamma(4000, shape=20, rate=7) # saving 4000 sampled values from a gamma distribution with a=20 and b=7 in lambda_posteror
lambda_posteror
quantile(lambda_posteror, probs = c(0.025, 0.975)) # finding the values at the quantiles 0.025, 0.975
# loading data on reaction times (rt), only one feature
library(bcogsci)
data("df_spacebar")
head(df_spacebar, n=2) # displaying head of df
# for comparision: frequentist linear model
m <-lm(rt~1, df_spacebar)
coef(m) # mean of intercept in form of a maximum likelihood estimate
sigma(m) # mean residual error in form of a maximum likelihood estimate
mean(df_spacebar$rt) # returns exaclty the same numer as coef(m)
sd(df_spacebar$rt) # returns exaclty the same numer as sigma(m)
# starting with unrealistic priors from a uniform distribution
library(brms)
fit_press <- brm(rt~1,
data=df_spacebar,
family=gaussian(), # likelihood we're assuming for the data shall be the gaussian likelihood (normal distribution)
prior=c(
prior(uniform(0,60000), class=Intercept, lb=0, ub=60000), # assuming mu (mean) comes from a prior uniform distribution between 0 and 60000
prior(uniform(0,2000), class=sigma, lb=0, ub=2000) # assuming sigma (sd) comes from a prior uniform distribution between 0 and 2000
),
chains= 4, # number of independent runs for sampling, default is 4
iter=2000, # rumber of iterations that the sampler makes to sample from the posterior distribution, default is 2000
warmup=1000 # number of iterations from the start of sampling that are eventually discarded, default is 1000 (half of iter)
)
# plotting posterior distributions of mu and sigma (and the chains)
plot(fit_press)
library(bayesplot)
library(magrittr)
as_draws_df(fit_press) %>% head(3)
as_draws_df(fit_press)$b_Intercept %>% mean()
as_draws_df(fit_press)$b_Intercept %>%  quantile(c(0.025, .975))
as_draws_df(fit_press)$sigma %>% mean() # mean of the Intercepts from all iterations
as_draws_df(fit_press)$sigma %>%  quantile(c(0.025, .975)) # confidence interval for 95% confidence
# once we have defined a bayesian model (containing the traditional model AND the prior beliefs), we can already generate a prior predictive distribution from it:
mu <- runif(1, min=0, max=60000) # sample from prior distribution from mu
sigma <- runif(1,0,2000) # sample from prior distribution from sigma
y_pred_1 <- rnorm(n=5, mu, sigma) # plugging those samples into a PDF/PMF to generate a data set y_pred
y_pred_1
# posterior distributions
# the posterior predictive distribution is a collection of data sets generated from the model
pp_check(fit_press, ndraws=100, type="dens_overlay")
# generating data from a lognormal:
mu <- 6
sigma <- 0.5
N <- 500000
sl <- rlnorm(N, mu, sigma)
sl
# generating prior predictive distributions
N_samples <- 1000
N_obs <- nrow(df_spacebar)
mu_samples <- runif(N_samples, 0, 11) # scaled down from 60000 to logarithmic scale
sigma_samples <- runif(N_samples, 0, 1) # scaled down from 2000 to logarithmic scale
# generating prior predictive distributions
N_samples <- 1000
N_obs <- nrow(df_spacebar)
mu_samples <- runif(N_samples, 0, 11) # scaled down from 60000 to logarithmic scale
sigma_samples <- runif(N_samples, 0, 1) # scaled down from 2000 to logarithmic scale
prior_pred_ln <- normal_predictive_distribution(
mu_samples = mu_samples,
sigma_samples = sigma_samples,
N_obs = N_obs
) %>% mutate(rt_pred = exp(rt_pred))
install.packages("tidyverse")
# generating prior predictive distributions
library(tidyverse) # for function "mutate"
N_samples <- 1000
N_obs <- nrow(df_spacebar)
mu_samples <- runif(N_samples, 0, 11) # scaled down from 60000 to logarithmic scale
sigma_samples <- runif(N_samples, 0, 1) # scaled down from 2000 to logarithmic scale
prior_pred_ln <- normal_predictive_distribution(
mu_samples = mu_samples,
sigma_samples = sigma_samples,
N_obs = N_obs
) %>% mutate(rt_pred = exp(rt_pred))
# generating prior predictive distributions
library(tidyverse) # for function "mutate"
# prior predictive distribution
fit_prior_press_ln <- brm(rt~1,
data = df_spacebar_ref,
family = lognormal(),
prior = c(
prior(normal(6,1.5), class = Intercept),
prior(normal(0,1), class = sigma)
),
sample_prior = "only",
controll = list(adapt_delta = .9)
)
# prior predictive distribution
fit_prior_press_ln <- brm(rt~1,
data = df_spacebar,
family = lognormal(),
prior = c(
prior(normal(6,1.5), class = Intercept),
prior(normal(0,1), class = sigma)
),
sample_prior = "only", # do not consider data, only sample from prior
controll = list(adapt_delta = .9)
)
# prior predictive distribution
fit_prior_press_ln <- brm(rt~1,
data = df_spacebar,
family = lognormal(),
prior = c(
prior(normal(6,1.5), class = Intercept),
prior(normal(0,1), class = sigma)
),
sample_prior = "only", # do not consider data, only sample from prior
control = list(adapt_delta = .9)
)
# prior predictive distribution
fit_prior_press_ln <- brm(rt~1,
data = df_spacebar,
family = lognormal(),
prior = c(
prior(normal(6,1.5), class = Intercept),
prior(normal(0,1), class = sigma)
),
sample_prior = "only", # do not consider data, only sample from prior
control = list(adapt_delta = .9)
)
# plotting posterior distributions of mu and sigma (and the chains)
plot(fit_prior_press_ln)
# plotting posterior distributions of mu and sigma (and the chains)
plot(fit_prior_press_ln)
# fitting model to data
fit_press_ln <- brm(rt~1,
data = df_spacebar,
family = lognormal(),
prior = c(
prior(normal(6,1.5), class = Intercept),
prior(normal(0,1), class = sigma)
)
)
# plotting posterior distributions of mu and sigma (and the chains)
plot(fit_press_ln)
# fitting model to data
fit_press_ln <- brm(rt~1,
data = df_spacebar,
family = lognormal(),
prior = c(
prior(normal(6,1.5), class = Intercept),
prior(normal(0,1), class = sigma)
)
)
fit_press_ln
# back-transforming to milliseconds
estimate_ms <- exp(as_draws_df((fit_press_ln)$b_Intercept))
fit_press_ln # prints out some summary statistics
# plotting posterior distributions of mu and sigma (and the chains)
plot(fit_press_ln)
# back-transforming to milliseconds
estimate_ms <- exp(as_draws_df(fit_press_ln)$b_Intercept)
estimate_ms
c(mean = mean(estimate_ms))
c(mean = mean(estimate_ms), quantile(estimate_ms, probs = c(.025, .975)))
# Question: Fit the model fit_press with just a few iterations, say 50 iterations
# (set warmup to the default of 25, and use four chains). Does the model converge?
fit_press <- brm(rt~1,
data=df_spacebar,
family=gaussian(), # likelihood we're assuming for the data shall be the gaussian likelihood (normal distribution)
prior=c(
prior(uniform(0,60000), class=Intercept, lb=0, ub=60000), # assuming mu (mean) comes from a prior uniform distribution between 0 and 60000
prior(uniform(0,2000), class=sigma, lb=0, ub=2000) # assuming sigma (sd) comes from a prior uniform distribution between 0 and 2000
),
chains= 4, # number of independent runs for sampling, default is 4
iter=50, # rumber of iterations that the sampler makes to sample from the posterior distribution, default is 2000
warmup=25 # number of iterations from the start of sampling that are eventually discarded, default is 1000 (half of iter)
)
plot(fit_press)
as_draws_df(fit_press) %>% head(3) # summary from 3 interations, 1st chain
as_draws_df(fit_press)$b_Intercept %>% mean() # mean of the Intercepts from all iterations
as_draws_df(fit_press)$b_Intercept %>%  quantile(c(0.025, .975)) # confidence interval for 95% confidence
as_draws_df(fit_press)$sigma %>% mean() # mean of the Intercepts from all iterations
as_draws_df(fit_press)$sigma %>%  quantile(c(0.025, .975))
# prior predictive distribution
fit_prior_press_ln <- brm(rt~1,
data = df_spacebar,
family = lognormal(),
prior = c(
prior(normal(6,1.5), class = Intercept),
prior(normal(0,1), class = sigma)
),
sample_prior = "only", # do not consider data, only sample from prior
control = list(adapt_delta = .9)
)
# getting mean mu and sd
as_draws_df(fit_press)$b_Intercept %>% mean() # mean of the Intercepts from all iterations
as_draws_df(fit_press)$sigma %>% mean()
exp(60000)
ln(60000)
fit_prior_press_B_ln <- brm(rt~1,
data = df_spacebar,
family = lognormal(),
prior = c(
prior(normal(6,1.5), class = Intercept),
prior(normal(0,1), class = sigma)
),
sample_prior = "only", # do not consider data, only sample from prior
control = list(adapt_delta = .9)
)
fit_prior_press_B_ln <- brm(rt~1,
data = df_spacebar,
family = lognormal(),
prior = c(
prior(normal(7,0.5), class = Intercept),
prior(normal(0,1), class = sigma)
),
sample_prior = "only", # do not consider data, only sample from prior
control = list(adapt_delta = .9)
)
as_draws_df(fit_press)$b_Intercept %>% mean() # mean of the Intercepts from all iterations
estimate_ms <- exp(as_draws_df(fit_press_ln)$b_Intercept) # exponent of all the samples for intercept
as_draws_df(fit_prior_press_B_ln)$b_Intercept %>% mean() # mean of the Intercepts from all iterations
estimate_ms <- exp(as_draws_df(fit_prior_press_B_ln)$b_Intercept) # exponent of all the samples for intercept
c(mean = mean(estimate_ms), quantile(estimate_ms, probs = c(.025, .975)))
fit_prior_press_B_ln <- brm(rt~1,
data = df_spacebar,
family = lognormal(),
prior = c(
prior(normal(9,1), class = Intercept),
prior(normal(0,1), class = sigma)
),
sample_prior = "only", # do not consider data, only sample from prior
control = list(adapt_delta = .9)
)
as_draws_df(fit_prior_press_B_ln)$b_Intercept %>% mean() # mean of the Intercepts from all iterations
estimate_ms <- exp(as_draws_df(fit_prior_press_B_ln)$b_Intercept) # exponent of all the samples for intercept
c(mean = mean(estimate_ms), quantile(estimate_ms, probs = c(.025, .975)))
fit_prior_press_B_ln <- brm(rt~1,
data = df_spacebar,
family = lognormal(),
prior = c(
prior(normal(1000,1000), class = Intercept),
prior(normal(0,1), class = sigma)
),
sample_prior = "only", # do not consider data, only sample from prior
control = list(adapt_delta = .9)
)
as_draws_df(fit_prior_press_B_ln)$b_Intercept %>% mean() # mean of the Intercepts from all iterations
estimate_ms <- exp(as_draws_df(fit_prior_press_B_ln)$b_Intercept) # exponent of all the samples for intercept
c(mean = mean(estimate_ms), quantile(estimate_ms, probs = c(.025, .975)))
